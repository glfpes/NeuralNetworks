\documentclass[final,authoryear,3p,times,onecolumn]{elsarticle}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{indentfirst}
\usepackage{amsthm}
\usepackage{cite}
\usepackage{bbding}
\usepackage{array}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage[lined,boxed,commentsnumbered,ruled]{algorithm2e}
\usepackage[colorlinks,urlcolor=blue]{hyperref}
\usepackage[all,cmtip]{xy}
\usepackage{multicol}
\usepackage{color}
\usepackage{subfigure}

\geometry{left=2cm,right=2cm,top=2.5cm,bottom=2.5cm}
\journal{Artificial Neural Networks}

\theoremstyle{definition}
\newtheorem{prop}{Proposition}
\newcommand{\ie}{{\em i.e. }}



\begin{document}

\begin{frontmatter}
    \title{\textbf{HW1 of Neural Networks}}
    \author{Yang Guang 1140339072}
\end{frontmatter}




\section{Derive the BP algorithm for MLQPs in both online learning and batch learning ways.}
k: the layer of the neural networks.(from 1 to 3)


$v_{kj}=\sum\limits_{i=1}^{N_{k-1}}(u_{kji}x_{k-1,i}^2+v_{kji}x_{k-1,i})+b_{kj}$, it is the input of neuron j in the k layer.

$x_{kj}=f(v_{kj})$, it is the output of neuron j in the k layer.

$e_{kj}=d_{kj}-x_{kj}$, it is the error of the output compared to label of neuron in the k layer.


\subsection{\bf{Online Learning}}
\paragraph{
For online learning, each iteration use a training example, and for the $n_{th}$ iteration, the total instantaneous error energy is $\epsilon(n)=\frac{1}{2}\sum\limits_{j \in C}e^2_j(n)$, here C stands for all the neurons in the output layer.
\newline \indent So the local gradient for output neuron is $\delta_j(n)=e_j(n)f'_j(v_j(n))$.
\newline \indent And for hidden neurons is $\delta_j(n)=f'_j(v_j(n))\sum\limits_K\delta_k(2u_kx_k+v_k)$, here k stands for single successor neuron connected to hidden neuron j and K stands for all successor neurons connected to hidden neuron j.
\newline \indent So we can get the change of weights as the following formula:
\newline \indent $\Delta u_{ji}(n)=a_1\Delta u_{ji}(n-1) + \eta_1 \delta_j(n) x^2_i(n)$
\newline \indent $\Delta v_{ji}(n)=a_2\Delta v_{ji}(n-1) + \eta_2 \delta_j(n) x_i(n)$}

\paragraph{
Thus, we can derive the algorithm as follows:
\newline \indent Step1, Initialize the weights of u and v randomly.
\newline \indent Step2, For each training example n, calculate its $x_{kj}$ of the output neuron as the forward pass, and update u and v using given formula below:
\newline \indent $u(n+1) = u(n) + \Delta u$
\newline \indent $v(n+1) = v(n) + \Delta v$
\newline \indent Step3, Repeat Step2 until convergence}

\subsection{\bf{Batch Learning}}
\paragraph{
For batch back propagation, the adjustment delta values are accumulated over all training items, to give an aggregate set of deltas, and then the aggregated deltas are applied to each weight and bias.
\newline \indent So we can derive the algorithm for batch as follows:
\newline \indent Step1, Initialize the weights of u and v randomly.
\newline \indent Step2, For each training example n, calculate its $x_{kj}$ of the output neuron as the forward pass, and calculate $\Delta u$ and $\Delta v$ for each iteration, then add them to $\Delta U$ and $\Delta V$
\newline \indent Step3, modify u and v by:
\newline \indent $u = u + \Delta U$
\newline \indent $v = v + \Delta V$
}

\section{Program Illustration}
\paragraph{
There are two Matlab programs, $BP\_batch.m$ and $BP\_online.m$, respectively implements the training and testing procedure of Batch Learning and Online Learning of back propagation algorithm
}


\section{Performance of the two ways}
\paragraph{The performance can be viewed at Figure 1 below. The ans parameter is the average error of the forecasted data and the given label}
\subsection{\bf{Processing Time}}
\paragraph{
The process time of the two methods of back propagation runs similar in this test case.
\newline \indent I think this is because the testing file is not large sufficient to discriminate the difference of running time. Since batch method needs to load all training examples, in theory it will take a longer time to process with these data compared with online method.
}
\subsection{\bf{Precision}}
\paragraph{
By compute the average error between the forecasted result and given label, we can see the performance of precision of batch and online BP algorithm.
\newline \indent  In the experiment displayed in the figure, We can find the precision of online training fits within $5$ percentage and $27$ percentage of batch method.
}



\begin{figure}[h]

           \center
           \scalebox{1.0}{\includegraphics{online.png}}
           \scalebox{1.0}{\includegraphics{batch.png}}
           \normalsize
           \caption{Performance for BP}
\end{figure}
\end{document}
